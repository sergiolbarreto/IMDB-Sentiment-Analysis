{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c9f888a6",
      "metadata": {
        "id": "c9f888a6"
      },
      "source": [
        "\n",
        "# An√°lise de Sentimentos no IMDb com Transformers\n",
        "\n",
        "**Autores:** S√©rgio Barreto (slbp) e Isaac Ferreira Silva (ifs5)\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Defini√ß√£o do Problema\n",
        "\n",
        "**Tarefa:** Classificar cr√≠ticas de filmes do IMDb em **positivas** (1) ou **negativas** (0) usando modelos *Transformers*.  \n",
        "**Objetivo desta etapa:** Definir claramente a aplica√ß√£o, treinar um **baseline reprodut√≠vel** e apresentar **resultados parciais**.  \n",
        "**Extens√µes inclu√≠das neste notebook:**  \n",
        "- **Pr√©-processamento** textual inicial;  \n",
        "- **Treinamento baseline** (DistilBERT por padr√£o);  \n",
        "- **Busca de hiperpar√¢metros com Optuna** para refinar *learning rate*, *epochs*, *batch size*, etc.;  \n",
        "- **Relato conciso de resultados** (acur√°cia, F1 e matriz de confus√£o).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df918c5",
      "metadata": {
        "id": "3df918c5"
      },
      "source": [
        "\n",
        "## 2) Depend√™ncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d08a09",
      "metadata": {
        "id": "e3d08a09"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets scikit-learn torch\n",
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KvKf4Us7UQZH",
      "metadata": {
        "id": "KvKf4Us7UQZH"
      },
      "source": [
        "## Carregando modelo j√° treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rR1D8CJoUGva",
      "metadata": {
        "id": "rR1D8CJoUGva"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "zip_path = \"/content/imdb_model.zip\"\n",
        "extract_dir = \"./melhor_modelo_imdb\"\n",
        "\n",
        "# 1) Extrair o zip\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "        print(f\"Arquivos extra√≠dos em: {extract_dir}\")\n",
        "else:\n",
        "    print(f\"Pasta {extract_dir} j√° existe, pulando extra√ß√£o.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h0VySEmjUKBT",
      "metadata": {
        "id": "h0VySEmjUKBT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_dir = \"./melhor_modelo_imdb/imdb_model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "print(\"Modelo e tokenizer carregados de:\", model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138c0d89",
      "metadata": {
        "id": "138c0d89"
      },
      "source": [
        "\n",
        "## 3) Configura√ß√£o do Experimento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61a4515",
      "metadata": {
        "id": "f61a4515"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Modelo\n",
        "    model_name: str = \"distilbert-base-uncased\"\n",
        "    max_length: int = 256\n",
        "    # Baseline training\n",
        "    epochs: int = 3\n",
        "    lr: float = 2e-5\n",
        "    train_bs: int = 16\n",
        "    eval_bs: int = 16\n",
        "    grad_accum_steps: int = 2\n",
        "    seed: int = 42\n",
        "    fp16: bool = True\n",
        "    N_TRAIN: Optional[int] = None\n",
        "    N_TEST: Optional[int]  = None\n",
        "    use_optuna: bool = True\n",
        "    n_trials: int = 10\n",
        "    N_TRAIN_HPO: int = 6000\n",
        "    N_VAL_HPO: int = 2000\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210e7eb8",
      "metadata": {
        "id": "210e7eb8"
      },
      "source": [
        "\n",
        "## 4) Importa√ß√µes, Ambiente e Semente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f6836fc",
      "metadata": {
        "id": "3f6836fc"
      },
      "outputs": [],
      "source": [
        "import re, random, numpy as np, torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
        "                          DataCollatorWithPadding, EarlyStoppingCallback)\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(cfg.seed)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcc35ec",
      "metadata": {
        "id": "5dcc35ec"
      },
      "source": [
        "\n",
        "## 5) Carregamento do Dataset IMDb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445c26e3",
      "metadata": {
        "id": "445c26e3"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
        "train_raw = dataset[\"train\"]\n",
        "test_raw  = dataset[\"test\"]\n",
        "if cfg.N_TRAIN is not None:\n",
        "    train_raw = train_raw.shuffle(seed=cfg.seed).select(range(cfg.N_TRAIN))\n",
        "if cfg.N_TEST is not None:\n",
        "    test_raw = test_raw.shuffle(seed=cfg.seed).select(range(cfg.N_TEST))\n",
        "\n",
        "len(train_raw), len(test_raw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c83b3a",
      "metadata": {
        "id": "e5c83b3a"
      },
      "source": [
        "\n",
        "## 6) Pr√©-processamento (Leve)\n",
        "\n",
        "Transformers funcionam bem com texto quase bruto, mas aplicamos **limpezas leves** e **sanidade de tamanho**:\n",
        "- remo√ß√£o de tags HTML simples;\n",
        "- normaliza√ß√£o de espa√ßos;\n",
        "- *clipping* de tamanho por tokeniza√ß√£o (feito na etapa de tokeniza√ß√£o).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323917f4",
      "metadata": {
        "id": "323917f4"
      },
      "outputs": [],
      "source": [
        "_html_tag = re.compile(r\"<[^>]+>\")\n",
        "_spaces = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = _html_tag.sub(\" \", s)\n",
        "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "    s = _spaces.sub(\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def apply_clean(ds):\n",
        "    return ds.map(lambda x: {\"text\": clean_text(x[\"text\"])}, batched=False)\n",
        "\n",
        "train_clean = apply_clean(train_raw)\n",
        "test_clean  = apply_clean(test_raw)\n",
        "\n",
        "# Estat√≠sticas simples de tamanho (caracteres)\n",
        "train_lens = [len(x[\"text\"]) for x in train_clean.select(range(min(2000, len(train_clean))))]\n",
        "test_lens  = [len(x[\"text\"]) for x in test_clean.select(range(min(2000, len(test_clean))))]\n",
        "\n",
        "print(\"Exemplo limpo:\", train_clean[0][\"text\"][:200], \"...\")\n",
        "print(\"Tamanhos (amostra) - train/test:\", (np.mean(train_lens), np.mean(test_lens)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b927801",
      "metadata": {
        "id": "6b927801"
      },
      "source": [
        "\n",
        "## 7) Tokeniza√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bcb267c",
      "metadata": {
        "id": "9bcb267c"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=False, max_length=cfg.max_length)\n",
        "\n",
        "train_tok = train_clean.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "test_tok  = test_clean.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44df7b17",
      "metadata": {
        "id": "44df7b17"
      },
      "source": [
        "\n",
        "## 8) M√©tricas e Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73155e1",
      "metadata": {
        "id": "e73155e1"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds)}\n",
        "\n",
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(cfg.model_name, num_labels=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e69456c8",
      "metadata": {
        "id": "e69456c8"
      },
      "source": [
        "\n",
        "## 9) Treinamento Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fa8eb1",
      "metadata": {
        "id": "97fa8eb1"
      },
      "outputs": [],
      "source": [
        "fp16_flag = cfg.fp16 and (device == \"cuda\")\n",
        "\n",
        "args_base = TrainingArguments(\n",
        "    output_dir=\"./results_baseline\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=cfg.epochs,\n",
        "    per_device_train_batch_size=cfg.train_bs,\n",
        "    per_device_eval_batch_size=cfg.eval_bs,\n",
        "    gradient_accumulation_steps=cfg.grad_accum_steps,\n",
        "    learning_rate=cfg.lr,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=fp16_flag\n",
        ")\n",
        "\n",
        "trainer_base = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=args_base,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=test_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "out_base = trainer_base.train()\n",
        "metrics_base = trainer_base.evaluate()\n",
        "metrics_base\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857e3c51",
      "metadata": {
        "id": "857e3c51"
      },
      "source": [
        "\n",
        "## 10) Busca de Hiperpar√¢metros com Optuna\n",
        "\n",
        "Para acelerar, usamos subconjuntos menores durante a busca (**HPO**).  \n",
        "Depois, **re-treinamos** com os melhores hiperpar√¢metros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jrLxLCOoLR8r",
      "metadata": {
        "id": "jrLxLCOoLR8r"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23eb58b4",
      "metadata": {
        "id": "23eb58b4"
      },
      "outputs": [],
      "source": [
        "best_params = None\n",
        "if cfg.use_optuna:\n",
        "    hpo_train = train_clean.shuffle(seed=cfg.seed).select(range(min(cfg.N_TRAIN_HPO, len(train_clean))))\n",
        "    hpo_val   = test_clean.shuffle(seed=cfg.seed).select(range(min(cfg.N_VAL_HPO, len(test_clean))))\n",
        "    hpo_train_tok = hpo_train.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "    hpo_val_tok   = hpo_val.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    def hp_space(trial):\n",
        "        return {\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
        "            \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n",
        "            \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
        "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.2),\n",
        "            \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.2),\n",
        "        }\n",
        "\n",
        "    args_hpo = TrainingArguments(\n",
        "        output_dir=\"./results_hpo\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        per_device_eval_batch_size=cfg.eval_bs,\n",
        "        gradient_accumulation_steps=cfg.grad_accum_steps,\n",
        "        logging_dir=\"./logs_hpo\",\n",
        "        report_to=\"none\",\n",
        "        fp16=fp16_flag\n",
        "    )\n",
        "\n",
        "    trainer_hpo = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=args_hpo,\n",
        "        train_dataset=hpo_train_tok,\n",
        "        eval_dataset=hpo_val_tok,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    best_run = trainer_hpo.hyperparameter_search(\n",
        "        direction=\"maximize\",\n",
        "        backend=\"optuna\",\n",
        "        hp_space=hp_space,\n",
        "        n_trials=cfg.n_trials\n",
        "    )\n",
        "\n",
        "    best_params = best_run.hyperparameters\n",
        "    best_params\n",
        "else:\n",
        "    print(\"HPO desativado; pulando Optuna.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "448cac4e",
      "metadata": {
        "id": "448cac4e"
      },
      "source": [
        "\n",
        "## 11) Re-Treinamento com Melhores Hiperpar√¢metros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7329dcce",
      "metadata": {
        "id": "7329dcce"
      },
      "outputs": [],
      "source": [
        "metrics_best = None\n",
        "trainer_best = None\n",
        "\n",
        "if best_params is not None:\n",
        "    # Monta novos argumentos de treino com melhores hiperpar√¢metros\n",
        "    args_best = TrainingArguments(\n",
        "        output_dir=\"./results_best\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=int(best_params.get(\"num_train_epochs\", cfg.epochs)),\n",
        "        per_device_train_batch_size=int(best_params.get(\"per_device_train_batch_size\", cfg.train_bs)),\n",
        "        per_device_eval_batch_size=cfg.eval_bs,\n",
        "        gradient_accumulation_steps=cfg.grad_accum_steps,\n",
        "        learning_rate=float(best_params.get(\"learning_rate\", cfg.lr)),\n",
        "        weight_decay=float(best_params.get(\"weight_decay\", 0.0)),\n",
        "        warmup_ratio=float(best_params.get(\"warmup_ratio\", 0.0)),\n",
        "        logging_dir=\"./logs_best\",\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=fp16_flag\n",
        "    )\n",
        "\n",
        "    trainer_best = Trainer(\n",
        "        model_init=model_init,\n",
        "        args=args_best,\n",
        "        train_dataset=train_tok,\n",
        "        eval_dataset=test_tok,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    out_best = trainer_best.train()\n",
        "    metrics_best = trainer_best.evaluate()\n",
        "\n",
        "metrics_best\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TDoV45rsvAHo",
      "metadata": {
        "id": "TDoV45rsvAHo"
      },
      "source": [
        "### Salvar e carregar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bh3x-d2QAi_2",
      "metadata": {
        "id": "bh3x-d2QAi_2"
      },
      "outputs": [],
      "source": [
        "# Definir o diret√≥rio onde o modelo ser√° salvo\n",
        "model_save_path = \"./melhor_modelo_imdb\"\n",
        "\n",
        "print(f\"Salvando o melhor modelo em: {model_save_path}\")\n",
        "\n",
        "# Salva o modelo (pesos)\n",
        "trainer_best.save_model(model_save_path)\n",
        "\n",
        "# √â crucial salvar o tokenizador tamb√©m para garantir que o pr√©-processamento seja id√™ntico\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(\"Modelo e tokenizador salvos com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fj7_8dhBqcWD",
      "metadata": {
        "collapsed": true,
        "id": "fj7_8dhBqcWD"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "zip_path = \"/content/imdb_model.zip\"\n",
        "extract_dir = \"./melhor_modelo_imdb\"\n",
        "\n",
        "# 1) Extrair o zip\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "        print(f\"Arquivos extra√≠dos em: {extract_dir}\")\n",
        "else:\n",
        "    print(f\"Pasta {extract_dir} j√° existe, pulando extra√ß√£o.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aVdazVu4sNXV",
      "metadata": {
        "id": "aVdazVu4sNXV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_dir = \"./melhor_modelo_imdb/imdb_model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "\n",
        "print(\"Modelo e tokenizer carregados de:\", model_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "571b9d34",
      "metadata": {
        "id": "571b9d34"
      },
      "source": [
        "\n",
        "## 12) Avalia√ß√£o Final e Matriz de Confus√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xb2iie6_WwpM",
      "metadata": {
        "id": "Xb2iie6_WwpM"
      },
      "outputs": [],
      "source": [
        "# Compara√ß√£o manual entre baseline e modelo otimizado\n",
        "if metrics_best is not None:\n",
        "    print(\"üìä Comparando baseline vs melhor modelo:\")\n",
        "    print(f\"Baseline -> Acur√°cia: {metrics_base['eval_accuracy']:.4f}, F1: {metrics_base['eval_f1']:.4f}\")\n",
        "    print(f\"Melhor (Optuna) -> Acur√°cia: {metrics_best['eval_accuracy']:.4f}, F1: {metrics_best['eval_f1']:.4f}\")\n",
        "\n",
        "    if metrics_best['eval_f1'] > metrics_base['eval_f1']:\n",
        "        print(\"‚úÖ O modelo otimizado com Optuna teve melhor desempenho.\")\n",
        "    else:\n",
        "        print(\"‚öôÔ∏è O baseline teve desempenho igual ou superior.\")\n",
        "else:\n",
        "    print(\"Usando apenas o modelo baseline (Optuna n√£o executado).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc00cfd",
      "metadata": {
        "id": "ebc00cfd"
      },
      "outputs": [],
      "source": [
        "_evaluator = trainer_base\n",
        "\n",
        "pred = _evaluator.predict(test_tok)\n",
        "y_true = pred.label_ids\n",
        "y_pred = pred.predictions.argmax(axis=1)\n",
        "\n",
        "print(\"=== Relat√≥rio de Classifica√ß√£o (Teste) ===\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"negativo\", \"positivo\"]))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "fig = plt.figure(figsize=(5,4))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title(\"Matriz de Confus√£o (Baseline)\")\n",
        "plt.xticks([0,1], [\"negativo\", \"positivo\"])\n",
        "plt.yticks([0,1], [\"negativo\", \"positivo\"])\n",
        "for (i, j), v in np.ndenumerate(cm):\n",
        "    plt.text(j, i, int(v), ha='center', va='center')\n",
        "plt.xlabel(\"Predito\")\n",
        "plt.ylabel(\"Verdadeiro\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25de9990",
      "metadata": {
        "id": "25de9990"
      },
      "source": [
        "\n",
        "## 13) Resultados Parciais\n",
        "\n",
        "O baseline com DistilBERT alcan√ßou desempenho consistente (acur√°cia e F1 elevados).\n",
        "Com Optuna, foram explorados hiperpar√¢metros-chave (learning rate, epochs, batch size, weight decay, warmup_ratio), e o melhor conjunto foi re-treinado no corpus completo, mantendo equil√≠brio entre classes na matriz de confus√£o.\n",
        "No entanto, o fine-tuning com Optuna n√£o superou o baseline, possivelmente porque o modelo original j√° estava bem ajustado ao dataset IMDb ‚Äî um corpus limpo, balanceado e de dom√≠nio est√°vel, no qual pequenos ajustes de hiperpar√¢metros t√™m impacto marginal. Al√©m disso, o espa√ßo de busca limitado e o baixo n√∫mero de trials reduziram a chance de encontrar combina√ß√µes significativamente melhores, e varia√ß√µes estat√≠sticas (como a semente aleat√≥ria e o particionamento dos dados) podem explicar diferen√ßas sutis.\n",
        "Esses resultados formam um baseline s√≥lido e est√°vel, servindo de ponto de partida confi√°vel para as pr√≥ximas etapas de robustez, interpretabilidade e ataques advers√°rios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IKpbshwV2dQe",
      "metadata": {
        "id": "IKpbshwV2dQe"
      },
      "source": [
        "## 14) Avalia√ß√£o de Robustez (Stress Testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1sR5Wkdz2qjQ",
      "metadata": {
        "id": "1sR5Wkdz2qjQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Configura√ß√µes Iniciais\n",
        "random.seed(42)\n",
        "model_path = \"./melhor_modelo_imdb/imdb_model\"\n",
        "\n",
        "print(f\"Carregando modelo de: {model_path}...\")\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "loaded_model.to(device)\n",
        "\n",
        "robustness_trainer = Trainer(model=loaded_model)\n",
        "\n",
        "# 3. Definir Fun√ß√µes de Perturba√ß√£o\n",
        "def perturb_typos(text, prob=0.05):\n",
        "    chars = list(text)\n",
        "    for i in range(len(chars) - 1):\n",
        "        if random.random() < prob:\n",
        "            chars[i], chars[i+1] = chars[i+1], chars[i]\n",
        "    return \"\".join(chars)\n",
        "\n",
        "def perturb_uppercase(text):\n",
        "    return text.upper()\n",
        "\n",
        "def perturb_spam_noise(text):\n",
        "    noises = [\" http://bit.ly/fake\", \" <br> CLICK HERE\", \" #ad #promo\"]\n",
        "    return text + \" \" + random.choice(noises)\n",
        "\n",
        "scenarios = {\n",
        "    \"Original\": lambda x: x,\n",
        "    \"Typos (5%)\": perturb_typos,\n",
        "    \"Caixa Alta (UPPER)\": perturb_uppercase,\n",
        "    \"Ru√≠do (Spam)\": perturb_spam_noise\n",
        "}\n",
        "\n",
        "# 4. Executar o Teste\n",
        "test_subset = test_clean.shuffle(seed=42).select(range(1000))\n",
        "\n",
        "print(\"\\n=== Resultados de Robustez (Modelo Carregado) ===\")\n",
        "print(f\"{'Cen√°rio':<25} | {'Acur√°cia':<10} | {'Diferen√ßa':<10}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Fun√ß√£o auxiliar para tokenizar dentro do loop (usando o tokenizador carregado)\n",
        "def tokenize_for_test(batch):\n",
        "    return loaded_tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=256)\n",
        "\n",
        "for name, func in scenarios.items():\n",
        "    # Aplica a perturba√ß√£o\n",
        "    perturbed_ds = test_subset.map(lambda x: {\"text\": func(x[\"text\"])}, batched=False)\n",
        "\n",
        "    # Tokeniza\n",
        "    perturbed_tok = perturbed_ds.map(tokenize_for_test, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    # Predi√ß√£o usando o modelo carregado\n",
        "    preds = robustness_trainer.predict(perturbed_tok)\n",
        "    y_pred = preds.predictions.argmax(axis=1)\n",
        "    y_true = preds.label_ids\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    results[name] = acc\n",
        "\n",
        "    # Calcular diferen√ßa para o original\n",
        "    diff = \"\"\n",
        "    if name != \"Original\":\n",
        "        delta = acc - results[\"Original\"]\n",
        "        diff = f\"{delta:.2%}\"\n",
        "\n",
        "    print(f\"{name:<25} | {acc:.4f}     | {diff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cqYKfOd2Q6pv",
      "metadata": {
        "id": "cqYKfOd2Q6pv"
      },
      "source": [
        "### 14.1) O que exatamente foi perturbado / quantidade de ru√≠do\n",
        "\n",
        "- **Typos (prob=0.05):** para cada par de caracteres adjacentes, h√° **5% de chance** de trocar a ordem (swap).  \n",
        "  *Observa√ß√£o:* isso gera **m√∫ltiplas trocas por review**, proporcional ao tamanho do texto.\n",
        "- **Caixa alta:** converte o texto inteiro para `UPPERCASE` (n√£o altera tokens sem√¢nticos, mas muda superf√≠cie).\n",
        "- **Ru√≠do/Spam:** **adiciona 1 sufixo** ao final do texto, escolhido aleatoriamente entre:\n",
        "  1) `http://bit.ly/fake`  2) `<br> CLICK HERE`  3) `#ad #promo`\n",
        "\n",
        "A seguir, al√©m de **acur√°cia**, vamos reportar:\n",
        "- **flip rate** (quantos exemplos mudam de classe vs. original)\n",
        "- **confian√ßa (softmax)** m√©dia do modelo (geral e apenas nos acertos)\n",
        "- **exemplos (antes/depois)** das transforma√ß√µes com predi√ß√£o + confian√ßa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xudVUElpQ6pv",
      "metadata": {
        "id": "xudVUElpQ6pv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _ensure_str_list(texts):\n",
        "    # pandas Series / numpy etc.\n",
        "    if hasattr(texts, \"tolist\"):\n",
        "        texts = texts.tolist()\n",
        "\n",
        "    # single example vira lista\n",
        "    if isinstance(texts, str):\n",
        "        return [texts]\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        texts = [texts]\n",
        "\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        if t is None:\n",
        "            out.append(\"\")\n",
        "        elif isinstance(t, str):\n",
        "            out.append(t)\n",
        "        elif isinstance(t, (list, tuple)):\n",
        "            out.append(\" \".join(map(str, t)))\n",
        "        else:\n",
        "            out.append(str(t))\n",
        "    return out\n",
        "\n",
        "def _predict_subset(texts, labels):\n",
        "    texts = _ensure_str_list(texts)\n",
        "    labels = list(labels) if hasattr(labels, \"__iter__\") else [labels]\n",
        "\n",
        "    tok = loaded_tokenizer(\n",
        "        texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = loaded_model(**tok).logits\n",
        "\n",
        "    probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    conf = probs.max(axis=1)\n",
        "    return y_pred, conf, probs, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z5rXqQ6j26_6",
      "metadata": {
        "id": "Z5rXqQ6j26_6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def safe_text(x):\n",
        "    return x if isinstance(x, str) else \"\"\n",
        "\n",
        "def tokenize_for_test(batch):\n",
        "    return loaded_tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=256)\n",
        "\n",
        "def predict_on_dataset(ds_tok):\n",
        "    out = robustness_trainer.predict(ds_tok)\n",
        "    logits = out.predictions\n",
        "    y_true = out.label_ids\n",
        "\n",
        "    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    conf = probs.max(axis=1)\n",
        "    return y_true, y_pred, conf, probs\n",
        "\n",
        "# subset\n",
        "test_subset = test_clean.shuffle(seed=42).select(range(1000))\n",
        "texts_orig = [safe_text(t) for t in test_subset[\"text\"]]\n",
        "\n",
        "tok_orig = test_subset.map(lambda x: {\"text\": safe_text(x[\"text\"])}, batched=False) \\\n",
        "                     .map(tokenize_for_test, batched=True, remove_columns=[\"text\"])\n",
        "y_true_orig, y_pred_orig, conf_orig, _ = predict_on_dataset(tok_orig)\n",
        "acc_orig = accuracy_score(y_true_orig, y_pred_orig)\n",
        "\n",
        "rows = []\n",
        "example_rows = []\n",
        "example_idx = [0, 1, 2]\n",
        "\n",
        "print(\"=== Resultados de Robustez (com confian√ßa + flips) ===\")\n",
        "\n",
        "for name, func in scenarios.items():\n",
        "    perturbed_ds = test_subset.map(lambda x: {\"text\": safe_text(func(safe_text(x[\"text\"])))}, batched=False)\n",
        "    perturbed_tok = perturbed_ds.map(tokenize_for_test, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    y_true, y_pred, conf, _ = predict_on_dataset(perturbed_tok)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    flip_rate = float((y_pred != y_pred_orig).mean())\n",
        "    mean_conf = float(conf.mean())\n",
        "    mean_conf_correct = float(conf[y_pred == y_true].mean()) if (y_pred == y_true).any() else float(\"nan\")\n",
        "\n",
        "    rows.append({\n",
        "        \"cenario\": name,\n",
        "        \"accuracy\": acc,\n",
        "        \"acc_delta_vs_orig\": acc - acc_orig,\n",
        "        \"flip_rate_vs_orig\": flip_rate,\n",
        "        \"mean_conf\": mean_conf,\n",
        "        \"mean_conf_delta_vs_orig\": mean_conf - float(conf_orig.mean()),\n",
        "        \"mean_conf_on_correct\": mean_conf_correct,\n",
        "    })\n",
        "\n",
        "    # exemplos antes/depois\n",
        "    texts_pert = [safe_text(func(t)) for t in texts_orig]\n",
        "    for j in example_idx:\n",
        "        example_rows.append({\n",
        "            \"cenario\": name,\n",
        "            \"orig_text\": texts_orig[j][:220].replace(\"\\n\",\" \"),\n",
        "            \"pert_text\": texts_pert[j][:220].replace(\"\\n\",\" \"),\n",
        "            \"y_true\": int(y_true[j]),\n",
        "            \"pred\": int(y_pred[j]),\n",
        "            \"conf\": float(conf[j]),\n",
        "            \"pred_orig\": int(y_pred_orig[j]),\n",
        "            \"conf_orig\": float(conf_orig[j]),\n",
        "        })\n",
        "\n",
        "df_rob = pd.DataFrame(rows).sort_values(\"accuracy\", ascending=False)\n",
        "display(df_rob)\n",
        "\n",
        "print(\"\\nExemplos (antes/depois) + predi√ß√£o + confian√ßa:\")\n",
        "df_examples = pd.DataFrame(example_rows)\n",
        "display(df_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nMHrh_IENLsw",
      "metadata": {
        "id": "nMHrh_IENLsw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_df = df_rob.copy()\n",
        "\n",
        "if (plot_df[\"cenario\"] == \"Original\").any():\n",
        "    plot_df[\"ord\"] = (plot_df[\"cenario\"] != \"Original\").astype(int)\n",
        "    plot_df = plot_df.sort_values([\"ord\", \"accuracy\"], ascending=[True, False])\n",
        "\n",
        "names = plot_df[\"cenario\"].tolist()\n",
        "values = plot_df[\"accuracy\"].tolist()\n",
        "\n",
        "original_acc = float(plot_df.loc[plot_df[\"cenario\"]==\"Original\",\"accuracy\"].iloc[0]) \\\n",
        "    if (plot_df[\"cenario\"]==\"Original\").any() else max(values)\n",
        "\n",
        "drops = [(original_acc - v) * 100 for v in values]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(names, values)\n",
        "\n",
        "plt.ylim(0, 1.1)\n",
        "plt.title(\"Robustez do Modelo sob Diferentes Cen√°rios\")\n",
        "plt.ylabel(\"Acur√°cia\")\n",
        "plt.axhline(y=original_acc, linestyle='--', alpha=0.5, label='Performance Original')\n",
        "\n",
        "for bar, drop in zip(bars, drops):\n",
        "    h = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, h, f'{h:.1%}\\n(-{drop:.1f}%)',\n",
        "             ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kh0EDV6iRICA",
      "metadata": {
        "id": "Kh0EDV6iRICA"
      },
      "source": [
        "## 14.2) Discuss√£o dos resultados de robustez (com m√©tricas objetivas)\n",
        "\n",
        "**Configura√ß√£o do stress test**\n",
        "- Avalia√ß√£o em **subset de 1000 reviews** do teste (para rapidez e reprodutibilidade).\n",
        "- Typos: `prob=0.05` de **swap de caracteres adjacentes**.\n",
        "- Caixa alta: `text.upper()`.\n",
        "- Ru√≠do/Spam: adiciona **1 sufixo** (entre 3 op√ß√µes) ao final da review.\n",
        "\n",
        "**O que reportar (m√≠nimo para n√£o ficar vago)**\n",
        "- **Acur√°cia** por cen√°rio e **Œî vs original**\n",
        "- **Flip rate** vs original (percentual de exemplos cuja classe muda)\n",
        "- **Confian√ßa (softmax)** m√©dia do modelo:\n",
        "  - geral\n",
        "  - somente nos acertos\n",
        "\n",
        "‚û°Ô∏è Use a tabela `df_rob` gerada acima como evid√™ncia principal e inclua 2‚Äì3 exemplos do `df_examples`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o3Gnit-x9kSC",
      "metadata": {
        "id": "o3Gnit-x9kSC"
      },
      "source": [
        "## 15) Avalia√ß√£o de Interpretabilidade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FMqiLIk5ZbO2",
      "metadata": {
        "id": "FMqiLIk5ZbO2"
      },
      "outputs": [],
      "source": [
        "model_path = \"./melhor_modelo_imdb/imdb_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Riq--vhBZdU7",
      "metadata": {
        "id": "Riq--vhBZdU7"
      },
      "outputs": [],
      "source": [
        "!pip install -q shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NkHci7UAeBgf",
      "metadata": {
        "id": "NkHci7UAeBgf"
      },
      "outputs": [],
      "source": [
        "loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rwCXvD9_Zkmj",
      "metadata": {
        "id": "rwCXvD9_Zkmj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "N_SHAP = 30\n",
        "MAX_LEN = 256\n",
        "SEED = 42\n",
        "\n",
        "shap_subset = test_clean.shuffle(seed=SEED).select(range(min(N_SHAP, len(test_clean))))\n",
        "texts_to_explain = [t if isinstance(t, str) else \"\" for t in shap_subset[\"text\"]]\n",
        "\n",
        "print(f\"Rodando SHAP em {len(texts_to_explain)} textos...\")\n",
        "\n",
        "device_id = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "sentiment_pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=loaded_model,\n",
        "    tokenizer=loaded_tokenizer,\n",
        "    return_all_scores=True,\n",
        "    device=device_id,\n",
        "    truncation=True,\n",
        "    max_length=MAX_LEN,\n",
        ")\n",
        "\n",
        "explainer = shap.Explainer(sentiment_pipe)\n",
        "shap_values = explainer(texts_to_explain)\n",
        "\n",
        "print(\"SHAP pronto. Gerando ranking global...\")\n",
        "\n",
        "def _token_list(x):\n",
        "    # shap_values.data costuma vir tokenizado ou em string\n",
        "    if isinstance(x, (list, np.ndarray)):\n",
        "        return list(x)\n",
        "    return str(x).split()\n",
        "\n",
        "def _vals_1d(v):\n",
        "    v = np.array(v)\n",
        "    if v.ndim == 1:\n",
        "        return v\n",
        "    if v.ndim == 2:\n",
        "        # usa a classe 1 (positivo) se existir; sen√£o pega a √∫ltima\n",
        "        if v.shape[1] >= 2:\n",
        "            return v[:, 1]\n",
        "        return v[:, -1]\n",
        "    return v.reshape(-1)\n",
        "\n",
        "signed_sum, abs_sum, cnt = {}, {}, {}\n",
        "\n",
        "for i in range(len(shap_values)):\n",
        "    toks = _token_list(shap_values.data[i])\n",
        "    vals = _vals_1d(shap_values.values[i])\n",
        "\n",
        "    m = min(len(toks), len(vals))\n",
        "    for t, s in zip(toks[:m], vals[:m]):\n",
        "        t = str(t).strip().lower()\n",
        "        if not t or not t.isalpha():\n",
        "            continue\n",
        "        signed_sum[t] = signed_sum.get(t, 0.0) + float(s)\n",
        "        abs_sum[t] = abs_sum.get(t, 0.0) + float(abs(s))\n",
        "        cnt[t] = cnt.get(t, 0) + 1\n",
        "\n",
        "df_shap_global = pd.DataFrame({\n",
        "    \"token\": list(cnt.keys()),\n",
        "    \"mean_shap\": [signed_sum[t] / cnt[t] for t in cnt.keys()],\n",
        "    \"mean_abs_shap\": [abs_sum[t] / cnt[t] for t in cnt.keys()],\n",
        "    \"n_occurrences\": [cnt[t] for t in cnt.keys()],\n",
        "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
        "\n",
        "MIN_OCC = 2\n",
        "df_shap_global_filt = df_shap_global[df_shap_global[\"n_occurrences\"] >= MIN_OCC].copy()\n",
        "\n",
        "# ‚ÄúImpacto consider√°vel‚Äù = top 5% por mean_abs_shap (agora com filtro)\n",
        "if len(df_shap_global_filt) > 0:\n",
        "    threshold = df_shap_global_filt[\"mean_abs_shap\"].quantile(0.95)\n",
        "    df_shap_global_filt[\"is_considerable\"] = df_shap_global_filt[\"mean_abs_shap\"] >= threshold\n",
        "    print(f\"Threshold (top 5%) em mean_abs_shap com n>={MIN_OCC}: {threshold:.6f}\")\n",
        "else:\n",
        "    threshold = None\n",
        "    print(\"Aviso: ap√≥s filtro n_occurrences>=2, n√£o sobrou token suficiente. Aumente N_SHAP.\")\n",
        "\n",
        "print(\"\\nTop tokens (global) ap√≥s filtro de ocorr√™ncia:\")\n",
        "display(df_shap_global_filt.head(20))\n",
        "\n",
        "i = 0\n",
        "toks0 = _token_list(shap_values.data[i])\n",
        "vals0 = _vals_1d(shap_values.values[i])\n",
        "m0 = min(len(toks0), len(vals0))\n",
        "df_local0 = pd.DataFrame({\"token\": toks0[:m0], \"shap\": vals0[:m0]})\n",
        "df_local0[\"abs_shap\"] = df_local0[\"shap\"].abs()\n",
        "df_local0 = df_local0.sort_values(\"abs_shap\", ascending=False).head(15)\n",
        "\n",
        "print(\"\\nExemplo local (top 15 tokens por |SHAP|) no texto 0:\")\n",
        "display(df_local0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y2Mu1sEVpAGj",
      "metadata": {
        "id": "y2Mu1sEVpAGj"
      },
      "source": [
        "## 16) Ataques Advers√°rios\n",
        "\n",
        "Nesta se√ß√£o avaliamos a vulnerabilidade do modelo a ataques advers√°rios em PLN.\n",
        "Diferente dos testes de robustez (ru√≠do, typos e caixa alta), aqui criamos\n",
        "entradas modificadas **intencionalmente** para tentar induzir o modelo a errar,\n",
        "com pequenas altera√ß√µes no texto.\n",
        "\n",
        "O foco √© responder:\n",
        "\n",
        "- O modelo muda de decis√£o com perturba√ß√µes bem pequenas?\n",
        "- Qual a taxa de exemplos cuja predi√ß√£o muda ap√≥s o ataque?\n",
        "- Esses ataques s√£o realistas do ponto de vista sem√¢ntico?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5lMnNDJEQ6pz",
      "metadata": {
        "id": "5lMnNDJEQ6pz"
      },
      "source": [
        "### 16.1) Defini√ß√µes + evid√™ncias (exemplos, como escolhemos ‚Äútokens emocionais‚Äù)\n",
        "\n",
        "- O dataset IMDb tem **r√≥tulo no n√≠vel da review** (positivo/negativo). **N√£o existe label por palavra**.  \n",
        "  Portanto, ‚Äúpalavra com carga emocional‚Äù aqui significa **tokens com polaridade forte** (alta val√™ncia), identificados **por heur√≠stica**.\n",
        "\n",
        "No notebook original, os gatilhos foram **manuais** (ex.: *terrible/awful* vs *excellent/wonderful*; *great/fantastic/wonderful*).  \n",
        "Para tornar isso ‚Äúaudit√°vel‚Äù, abaixo n√≥s:\n",
        "1) mostramos **exemplos reais** onde a predi√ß√£o **vira** (antes/depois + confian√ßa), e  \n",
        "2) extra√≠mos uma lista de tokens ‚Äúfortes‚Äù automaticamente via **SHAP agregado** (quando dispon√≠vel).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vq4h2E_tPFiI",
      "metadata": {
        "id": "Vq4h2E_tPFiI"
      },
      "outputs": [],
      "source": [
        "def adv_injection_opposite_sentiment(example):\n",
        "    \"\"\"\n",
        "    Ataque 1: injetar uma frase com sentimento oposto ao r√≥tulo.\n",
        "    label==1 (positivo) -> injeta termos negativos\n",
        "    label==0 (negativo) -> injeta termos positivos\n",
        "    \"\"\"\n",
        "    text = example.get(\"text\", \"\")\n",
        "    label = int(example.get(\"label\", 0))\n",
        "\n",
        "    if label == 1:\n",
        "        suffix = \" However, some people might say this movie is terrible and absolutely awful.\"\n",
        "    else:\n",
        "        suffix = \" However, some people might say this movie is excellent and absolutely wonderful.\"\n",
        "\n",
        "    return {\"text\": text + \" \" + suffix}\n",
        "\n",
        "def adv_trigger_neutral(example):\n",
        "    \"\"\"\n",
        "    Ataque 2: frase aparentemente neutra, mas com palavras polarizadas.\n",
        "    \"\"\"\n",
        "    text = example.get(\"text\", \"\")\n",
        "    trigger = (\n",
        "        \" This sentence is only for analysis and should not change the real opinion, \"\n",
        "        \"but it mentions that the movie is great, fantastic and wonderful.\"\n",
        "    )\n",
        "    return {\"text\": text + \" \" + trigger}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5NmWpHokQ6pz",
      "metadata": {
        "id": "5NmWpHokQ6pz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def _ensure_str_list(texts):\n",
        "\n",
        "    if isinstance(texts, str):\n",
        "        return [texts]\n",
        "\n",
        "    # datasets.Column geralmente tem to_pylist()\n",
        "    if hasattr(texts, \"to_pylist\"):\n",
        "        texts = texts.to_pylist()\n",
        "    elif hasattr(texts, \"tolist\"):\n",
        "        texts = texts.tolist()\n",
        "    elif not isinstance(texts, (list, tuple)) and hasattr(texts, \"__iter__\"):\n",
        "        texts = list(texts)\n",
        "\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        if t is None:\n",
        "            out.append(\"\")\n",
        "        elif isinstance(t, str):\n",
        "            out.append(t)\n",
        "        elif isinstance(t, (list, tuple, np.ndarray)):\n",
        "            out.append(\" \".join(map(str, t)))\n",
        "        else:\n",
        "            out.append(str(t))\n",
        "    return out\n",
        "\n",
        "\n",
        "def _predict_texts(texts, labels):\n",
        "    texts = _ensure_str_list(texts)\n",
        "    tok = loaded_tokenizer(\n",
        "        texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = loaded_model(**tok).logits\n",
        "    probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    conf = probs.max(axis=1)\n",
        "    return y_pred, conf, probs, labels\n",
        "\n",
        "def apply_attack_safely(ds, attack_fn):\n",
        "    def _wrap(ex):\n",
        "        out = attack_fn(ex)\n",
        "        # attack_fn pode retornar dict ou string; se n√£o retornar nada, usa o original\n",
        "        if isinstance(out, dict) and \"text\" in out:\n",
        "            t = out[\"text\"]\n",
        "        elif isinstance(out, str):\n",
        "            t = out\n",
        "        else:\n",
        "            t = ex.get(\"text\", \"\")\n",
        "        # sanitiza\n",
        "        if t is None:\n",
        "            t = \"\"\n",
        "        elif not isinstance(t, str):\n",
        "            if isinstance(t, (list, tuple, np.ndarray)):\n",
        "                t = \" \".join(map(str, t))\n",
        "            else:\n",
        "                t = str(t)\n",
        "        return {\"text\": t}\n",
        "    return ds.map(_wrap, batched=False)\n",
        "\n",
        "# Trabalhar com um subset pequeno para relat√≥rio\n",
        "N = 300\n",
        "subset_adv = test_subset.select(range(min(N, len(test_subset))))\n",
        "\n",
        "texts = _ensure_str_list(subset_adv[\"text\"])\n",
        "y_true_adv = np.array(subset_adv[\"label\"])\n",
        "\n",
        "# Original\n",
        "pred0, conf0, _, _ = _predict_texts(texts, y_true_adv)\n",
        "acc0 = (pred0 == y_true_adv).mean()\n",
        "print(f\"Acur√°cia (original) no subset: {acc0:.4f}\")\n",
        "\n",
        "def run_attack(attack_name, attack_fn, k_examples=5):\n",
        "    global texts_adv\n",
        "    adv_ds = apply_attack_safely(subset_adv, attack_fn)\n",
        "    texts_adv = _ensure_str_list(adv_ds[\"text\"])\n",
        "\n",
        "    pred1, conf1, _, _ = _predict_texts(texts_adv, y_true_adv)\n",
        "    acc1 = (pred1 == y_true_adv).mean()\n",
        "    flip = pred1 != pred0\n",
        "    flip_rate = flip.mean()\n",
        "    print(f\"\\n[{attack_name}] acc={acc1:.4f} | Œîacc={acc1-acc0:+.4f} | flip_rate={flip_rate:.2%}\")\n",
        "\n",
        "    idx = np.where(flip)[0]\n",
        "    if len(idx) == 0:\n",
        "        idx = np.argsort(conf0 - conf1)[-k_examples:]\n",
        "    else:\n",
        "        idx = idx[:k_examples]\n",
        "\n",
        "    rows = []\n",
        "    for i in idx:\n",
        "        rows.append({\n",
        "            \"i\": int(i),\n",
        "            \"y_true\": int(y_true_adv[i]),\n",
        "            \"pred_orig\": int(pred0[i]),\n",
        "            \"conf_orig\": float(conf0[i]),\n",
        "            \"pred_adv\": int(pred1[i]),\n",
        "            \"conf_adv\": float(conf1[i]),\n",
        "            \"delta_conf\": float(conf1[i] - conf0[i]),\n",
        "            \"orig_prefix\": texts[i][:220].replace(\"\\n\",\" \"),\n",
        "            \"adv_prefix\": texts_adv[i][:220].replace(\"\\n\",\" \"),\n",
        "            \"orig_suffix\": texts[i][-220:].replace(\"\\n\",\" \"),\n",
        "            \"adv_suffix\": texts_adv[i][-220:].replace(\"\\n\",\" \"),\n",
        "            \"len_orig\": len(texts[i]),\n",
        "            \"len_adv\": len(texts_adv[i]),\n",
        "\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_flip1 = run_attack(\"OppositeSentimentInjection\", adv_injection_opposite_sentiment)\n",
        "display(df_flip1)\n",
        "\n",
        "df_flip2 = run_attack(\"NeutralTriggerWithSentimentWords\", adv_trigger_neutral)\n",
        "display(df_flip2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXzcAK9KkA8i",
      "metadata": {
        "id": "uXzcAK9KkA8i"
      },
      "outputs": [],
      "source": [
        "import difflib\n",
        "\n",
        "def show_insert(i, texts_adv_local):\n",
        "    o = texts[i]\n",
        "    a = texts_adv_local[i]\n",
        "\n",
        "    sm = difflib.SequenceMatcher(None, o, a)\n",
        "    inserts = []\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"insert\":\n",
        "            inserts.append(a[j1:j2])\n",
        "\n",
        "    print(\"Inser√ß√µes encontradas:\")\n",
        "    for k, ins in enumerate(inserts[:5], 1):\n",
        "        print(f\"{k}. {ins.strip()[:400]}\")\n",
        "\n",
        "show_insert(int(df_flip1.iloc[0][\"i\"]), texts_adv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coiCuy0NNPbz",
      "metadata": {
        "id": "coiCuy0NNPbz"
      },
      "source": [
        "### Gr√°ficos e an√°lises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hAaxHcCYGvTF",
      "metadata": {
        "id": "hAaxHcCYGvTF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "FIG_DIR = \"figs\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def savefig(name):\n",
        "    path = os.path.join(FIG_DIR, name)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
        "    print(\"Salvo em:\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rPWzS7-WNSRW",
      "metadata": {
        "id": "rPWzS7-WNSRW"
      },
      "source": [
        "#### Robustez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n_6WXOxYGxAu",
      "metadata": {
        "id": "n_6WXOxYGxAu"
      },
      "outputs": [],
      "source": [
        "plot_df = df_rob.copy()\n",
        "\n",
        "if (plot_df[\"cenario\"] == \"Original\").any():\n",
        "    plot_df[\"ord\"] = (plot_df[\"cenario\"] != \"Original\").astype(int)\n",
        "    plot_df = plot_df.sort_values([\"ord\", \"accuracy\"], ascending=[True, False])\n",
        "\n",
        "names = plot_df[\"cenario\"].tolist()\n",
        "acc = plot_df[\"accuracy\"].tolist()\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.bar(names, acc)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.ylabel(\"Acur√°cia\")\n",
        "plt.title(\"Robustez: Acur√°cia por Cen√°rio\")\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "savefig(\"robustez_acuracia.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cLbNih6WNNtL",
      "metadata": {
        "id": "cLbNih6WNNtL"
      },
      "outputs": [],
      "source": [
        "if \"flip_rate_vs_orig\" in plot_df.columns:\n",
        "    plt.figure(figsize=(9,4))\n",
        "    plt.bar(names, plot_df[\"flip_rate_vs_orig\"].tolist())\n",
        "    plt.ylim(0, max(0.02, float(plot_df[\"flip_rate_vs_orig\"].max())*1.2))\n",
        "    plt.ylabel(\"Flip rate vs Original\")\n",
        "    plt.title(\"Robustez: Taxa de Invers√£o de Predi√ß√£o\")\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    savefig(\"robustez_fliprate.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aofHTRjGNVl0",
      "metadata": {
        "id": "aofHTRjGNVl0"
      },
      "source": [
        "#### Ataques Adversariais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H7Wn0biVNYGN",
      "metadata": {
        "id": "H7Wn0biVNYGN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_adv_plot = pd.DataFrame([\n",
        "    {\"ataque\":\"Original\", \"accuracy\": 0.9000, \"flip_rate\": 0.0},\n",
        "    {\"ataque\":\"OppositeSentimentInjection\", \"accuracy\": 0.7833, \"flip_rate\": 0.1167},\n",
        "    {\"ataque\":\"NeutralTrigger\", \"accuracy\": 0.8833, \"flip_rate\": 0.0567},\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.bar(df_adv_plot[\"ataque\"], df_adv_plot[\"accuracy\"])\n",
        "plt.ylim(0, 1.05)\n",
        "plt.ylabel(\"Acur√°cia\")\n",
        "plt.title(\"Ataques Adversariais: Acur√°cia\")\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "savefig(\"adv_acuracia.png\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.bar(df_adv_plot[\"ataque\"], df_adv_plot[\"flip_rate\"])\n",
        "plt.ylim(0, max(0.02, df_adv_plot[\"flip_rate\"].max()*1.2))\n",
        "plt.ylabel(\"Flip rate\")\n",
        "plt.title(\"Ataques Adversariais: Taxa de Invers√£o (flip rate)\")\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "savefig(\"adv_fliprate.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CMjFfIFuNbbh",
      "metadata": {
        "id": "CMjFfIFuNbbh"
      },
      "source": [
        "#### SHAP Global: top tokens por impacto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VAVm2VlpNeha",
      "metadata": {
        "id": "VAVm2VlpNeha"
      },
      "outputs": [],
      "source": [
        "topn = 15\n",
        "top = df_shap_global_filt.sort_values(\"mean_abs_shap\", ascending=False).head(topn).copy()\n",
        "top = top.sort_values(\"mean_abs_shap\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.barh(top[\"token\"], top[\"mean_abs_shap\"])\n",
        "plt.xlabel(\"mean(|SHAP|)\")\n",
        "plt.title(f\"SHAP Global: Top {topn} Tokens por Impacto M√©dio\")\n",
        "savefig(\"shap_top_tokens.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1TRCeIbrPpom",
      "metadata": {
        "id": "1TRCeIbrPpom"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# garante seed igual ao seu experimento de robustez\n",
        "random.seed(42)\n",
        "\n",
        "texts_orig = test_subset[\"text\"]\n",
        "typo_fn = scenarios[\"Typos (5%)\"]\n",
        "\n",
        "def count_char_diffs(a: str, b: str) -> int:\n",
        "    # conta posi√ß√µes diferentes + diferen√ßa de tamanho\n",
        "    m = min(len(a), len(b))\n",
        "    diff = sum(1 for i in range(m) if a[i] != b[i])\n",
        "    diff += abs(len(a) - len(b))\n",
        "    return diff\n",
        "\n",
        "# gera textos com typos\n",
        "texts_typos = [typo_fn(t) for t in texts_orig]\n",
        "\n",
        "# calcula m√©dia de caracteres alterados\n",
        "deltas = [count_char_diffs(o, p) for o, p in zip(texts_orig, texts_typos)]\n",
        "mean_delta_chars = float(np.mean(deltas))\n",
        "\n",
        "print(f\"Œî_chars m√©dio (Typos 5%): {mean_delta_chars:.2f} caracteres alterados por exemplo\")\n",
        "print(f\"Mediana: {float(np.median(deltas)):.0f} | Min: {int(np.min(deltas))} | Max: {int(np.max(deltas))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6GGHh32pdc0e",
      "metadata": {
        "id": "6GGHh32pdc0e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def _ensure_str_list(texts):\n",
        "    if isinstance(texts, str):\n",
        "        return [texts]\n",
        "    if hasattr(texts, \"to_pylist\"):\n",
        "        texts = texts.to_pylist()\n",
        "    elif hasattr(texts, \"tolist\"):\n",
        "        texts = texts.tolist()\n",
        "    elif not isinstance(texts, (list, tuple)) and hasattr(texts, \"__iter__\"):\n",
        "        texts = list(texts)\n",
        "\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        if t is None:\n",
        "            out.append(\"\")\n",
        "        elif isinstance(t, str):\n",
        "            out.append(t)\n",
        "        elif isinstance(t, (list, tuple, np.ndarray)):\n",
        "            out.append(\" \".join(map(str, t)))\n",
        "        else:\n",
        "            out.append(str(t))\n",
        "    return out\n",
        "\n",
        "def predict_with_conf(texts, y_true):\n",
        "    texts = _ensure_str_list(texts)\n",
        "    tok = loaded_tokenizer(texts, truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = loaded_model(**tok).logits\n",
        "    probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "    y_pred = probs.argmax(axis=1)\n",
        "    conf = probs.max(axis=1)\n",
        "    return y_pred, conf\n",
        "\n",
        "# subset para relat√≥rio\n",
        "N = 300\n",
        "subset_rep = test_subset.select(range(min(N, len(test_subset))))\n",
        "texts_orig = _ensure_str_list(subset_rep[\"text\"])\n",
        "y_true = np.array(subset_rep[\"label\"])\n",
        "\n",
        "# baseline\n",
        "y_pred0, conf0 = predict_with_conf(texts_orig, y_true)\n",
        "acc0 = accuracy_score(y_true, y_pred0)\n",
        "\n",
        "rows = []\n",
        "ex_rows = []\n",
        "\n",
        "# mesmos √≠ndices de exemplo em todos cen√°rios\n",
        "example_idx = [0, 1, 2]\n",
        "\n",
        "for name, fn in scenarios.items():\n",
        "    texts_pert = [fn(t) for t in texts_orig]\n",
        "    y_pred, conf = predict_with_conf(texts_pert, y_true)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    flip_rate = float((y_pred != y_pred0).mean())\n",
        "    mean_conf = float(conf.mean())\n",
        "    mean_conf_correct = float(conf[y_pred == y_true].mean()) if (y_pred == y_true).any() else float(\"nan\")\n",
        "\n",
        "    rows.append({\n",
        "        \"cenario\": name,\n",
        "        \"accuracy\": float(acc),\n",
        "        \"delta_acc_vs_orig\": float(acc - acc0),\n",
        "        \"flip_rate_vs_orig\": flip_rate,\n",
        "        \"mean_conf\": mean_conf,\n",
        "        \"mean_conf_on_correct\": mean_conf_correct,\n",
        "    })\n",
        "\n",
        "    for i in example_idx:\n",
        "        ex_rows.append({\n",
        "            \"cenario\": name,\n",
        "            \"orig_prefix\": texts_orig[i][:140].replace(\"\\n\", \" \"),\n",
        "            \"pert_prefix\": texts_pert[i][:140].replace(\"\\n\", \" \"),\n",
        "            \"y_true\": int(y_true[i]),\n",
        "            \"pred\": int(y_pred[i]),\n",
        "            \"conf\": float(conf[i]),\n",
        "            \"acerto\": bool(y_pred[i] == y_true[i]),\n",
        "        })\n",
        "\n",
        "df_rob = pd.DataFrame(rows)\n",
        "df_rob = df_rob.sort_values(\"cenario\")\n",
        "display(df_rob)\n",
        "\n",
        "df_rob_examples = pd.DataFrame(ex_rows)\n",
        "display(df_rob_examples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TTbRC56JX7ww",
      "metadata": {
        "id": "TTbRC56JX7ww"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "texts_orig = test_subset[\"text\"]\n",
        "spam_fn = scenarios[\"Ru√≠do (Spam)\"]\n",
        "\n",
        "texts_spam = [spam_fn(t) for t in texts_orig]\n",
        "\n",
        "def n_tokens(text: str) -> int:\n",
        "    # tokens do modelo (melhor do que split em palavras)\n",
        "    return len(loaded_tokenizer.encode(text, truncation=True, max_length=512))\n",
        "\n",
        "tok_orig = [n_tokens(t) for t in texts_orig]\n",
        "tok_spam = [n_tokens(t) for t in texts_spam]\n",
        "\n",
        "delta_tokens = [s - o for o, s in zip(tok_orig, tok_spam)]\n",
        "mean_delta_tokens = float(np.mean(delta_tokens))\n",
        "\n",
        "print(f\"Œî_tokens m√©dio (Spam): {mean_delta_tokens:.2f} tokens extras por exemplo (tokeniza√ß√£o do modelo)\")\n",
        "print(f\"Mediana: {float(np.median(delta_tokens)):.0f} | Min: {int(np.min(delta_tokens))} | Max: {int(np.max(delta_tokens))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4IptqmyJYBX8",
      "metadata": {
        "id": "4IptqmyJYBX8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def predict_with_conf(text_list):\n",
        "    tok = loaded_tokenizer(text_list, truncation=True, padding=True, max_length=256, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = loaded_model(**tok).logits\n",
        "    probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "    pred = probs.argmax(axis=1)\n",
        "    conf = probs.max(axis=1)\n",
        "    return pred, conf\n",
        "\n",
        "example_idx = [0, 1, 2]\n",
        "rows = []\n",
        "\n",
        "texts_orig = test_subset[\"text\"]\n",
        "y_true = np.array(test_subset[\"label\"])\n",
        "\n",
        "sc_to_show = [\"Typos (5%)\", \"Caixa Alta (UPPER)\", \"Ru√≠do (Spam)\"]\n",
        "\n",
        "for scen in sc_to_show:\n",
        "    fn = scenarios[scen]\n",
        "    orig_ex = [texts_orig[i] for i in example_idx]\n",
        "    pert_ex = [fn(texts_orig[i]) for i in example_idx]\n",
        "\n",
        "    pred, conf = predict_with_conf(pert_ex)\n",
        "    for k, i in enumerate(example_idx):\n",
        "        rows.append({\n",
        "            \"cenario\": scen,\n",
        "            \"orig_prefix\": orig_ex[k][:140].replace(\"\\n\", \" \"),\n",
        "            \"pert_prefix\": pert_ex[k][:140].replace(\"\\n\", \" \"),\n",
        "            \"y_true\": int(y_true[i]),\n",
        "            \"pred\": int(pred[k]),\n",
        "            \"conf\": float(conf[k]),\n",
        "            \"acerto\": bool(pred[k] == y_true[i]),\n",
        "        })\n",
        "\n",
        "df_rob_examples = pd.DataFrame(rows)\n",
        "display(df_rob_examples)\n",
        "\n",
        "for _, r in df_rob_examples.iterrows():\n",
        "    acerto = \"Sim\" if r[\"acerto\"] else \"N√£o\"\n",
        "    print(f\"{r['orig_prefix']} & {r['pert_prefix']} & {r['pred']} & {r['conf']:.3f} & {acerto} \\\\\\\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PErkUmahYIrW",
      "metadata": {
        "id": "PErkUmahYIrW"
      },
      "outputs": [],
      "source": [
        "import difflib\n",
        "import pandas as pd\n",
        "\n",
        "def extract_insertions(orig: str, adv: str, max_len=180):\n",
        "    sm = difflib.SequenceMatcher(None, orig, adv)\n",
        "    inserts = []\n",
        "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
        "        if tag == \"insert\":\n",
        "            chunk = adv[j1:j2].strip()\n",
        "            if chunk:\n",
        "                inserts.append(chunk)\n",
        "    if not inserts:\n",
        "        return \"\"\n",
        "    s = \" | \".join(inserts)\n",
        "    return (s[:max_len] + \"...\") if len(s) > max_len else s\n",
        "\n",
        "pick1 = df_flip1.head(2).copy()\n",
        "pick2 = df_flip2.head(1).copy()\n",
        "picked = pd.concat([pick1, pick2], ignore_index=True)\n",
        "\n",
        "rows = []\n",
        "for _, r in picked.iterrows():\n",
        "    i = int(r[\"i\"])\n",
        "    orig = texts[i]\n",
        "    adv = texts_adv[i]\n",
        "    ins = extract_insertions(orig, adv)\n",
        "\n",
        "    rows.append({\n",
        "        \"i\": i,\n",
        "        \"y_true\": int(r[\"y_true\"]),\n",
        "        \"pred0\": int(r[\"pred_orig\"]),\n",
        "        \"pred1\": int(r[\"pred_adv\"]),\n",
        "        \"conf0\": float(r[\"conf_orig\"]),\n",
        "        \"conf1\": float(r[\"conf_adv\"]),\n",
        "        \"dconf\": float(r[\"delta_conf\"]),\n",
        "        \"insercao\": ins\n",
        "    })\n",
        "\n",
        "df_adv_examples = pd.DataFrame(rows)\n",
        "display(df_adv_examples)\n",
        "\n",
        "for _, r in df_adv_examples.iterrows():\n",
        "    arrow = f\"{r['pred0']}\\\\rightarrow{r['pred1']}\"\n",
        "    ins = r[\"insercao\"].replace(\"&\", \"\\\\&\")\n",
        "    print(f\"{r['i']} & {r['y_true']} & {arrow} & {r['conf0']:.3f} & {r['conf1']:.3f} & {r['dconf']:+.3f} & \\\\textit{{{ins}}} \\\\\\\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EXuNgMg1YlV3",
      "metadata": {
        "id": "EXuNgMg1YlV3"
      },
      "outputs": [],
      "source": [
        "df_adv_examples = pd.concat([\n",
        "    df_flip1.assign(ataque=\"OppositeSentimentInjection\"),\n",
        "    df_flip2.assign(ataque=\"NeutralTriggerWithSentimentWords\"),\n",
        "]).head(6)\n",
        "\n",
        "display(df_adv_examples[[\n",
        "    \"ataque\",\"i\",\"y_true\",\"pred_orig\",\"pred_adv\",\"conf_orig\",\"conf_adv\",\"delta_conf\",\n",
        "    \"orig_suffix\",\"adv_suffix\"\n",
        "]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yr_JPMSUeTbm",
      "metadata": {
        "id": "yr_JPMSUeTbm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "os.makedirs(\"figs\", exist_ok=True)\n",
        "\n",
        "tmp = df_local0.head(12).copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,3.5))\n",
        "ax.axis(\"off\")\n",
        "table = ax.table(cellText=tmp.values, colLabels=tmp.columns, loc=\"center\")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(8)\n",
        "table.scale(1, 1.2)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figs/shap_local_ex0.png\", dpi=200)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RDx7sg70k0-K",
      "metadata": {
        "id": "RDx7sg70k0-K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
